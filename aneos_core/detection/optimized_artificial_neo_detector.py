"""
Optimized Artificial NEO Detector - Production Ready
CLAUDETTE SWARM DELTA - Final optimization based on validation results

Addresses GAMMA's concerns:
- Reduced false positive rate from 81.5% to target <20%
- Adjusted aggressive boost factors 
- Raised detection threshold from 0.4 to 0.6
- Refined scoring algorithms based on validation data
"""

import numpy as np
import math
from typing import Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class OptimizedDetectionResult:
    """Optimized detection result with production-ready parameters."""
    is_artificial: bool
    confidence: float
    analysis: Dict[str, Any]
    confidence_breakdown: Dict[str, float]

class OptimizedArtificialNEODetector:
    """Production-ready artificial NEO detector with optimized parameters."""
    
    def __init__(self, detection_threshold: float = 0.6):
        self.logger = logging.getLogger(__name__)
        self.detection_threshold = detection_threshold
        
        # Optimized scoring weights (reduced from aggressive 1.5x multipliers)
        self.weights = {
            'heliocentric_escape': 0.35,      # Reduced from 0.7
            'launch_azimuth': 0.25,           # Reduced from 0.4  
            'polar_orbit': 0.20,              # Reduced from 0.3
            'orbital_energy': 0.30,           # Reduced from 0.5
            'brightness_anomaly': 0.15,       # Reduced from 0.2
            'resonance': 0.10                 # Reduced from 0.15
        }
        
        # Optimized thresholds based on validation data
        self.thresholds = {
            'escape_au': 1.0,                 # Raised from 0.8 (more conservative)
            'low_inclination': 25,            # Reduced from 35 (more selective)
            'high_inclination': 135,          # Raised from 120 (more selective)
            'eccentricity': 0.35,             # Raised from 0.25 (more selective)
            'magnitude_deviation': 3.0,       # Raised from 2.0 (more selective)
            'resonance_tolerance': 0.05       # Reduced from 0.1 (more selective)
        }
    
    def analyze_neo(self, orbital_elements: Dict[str, float], physical_data: Dict[str, Any] = None) -> OptimizedDetectionResult:
        """Analyze NEO with optimized production-ready parameters."""
        
        analysis = {}
        confidence_breakdown = {}
        total_score = 0.0
        
        # Extract orbital parameters
        a = orbital_elements.get('a', 0)  # semi-major axis
        e = orbital_elements.get('e', 0)  # eccentricity  
        i = orbital_elements.get('i', 0)  # inclination
        
        if a == 0 or not orbital_elements:
            return OptimizedDetectionResult(False, 0.0, {"error": "no_orbital_data"}, {})
        
        # OPTIMIZED ANALYSIS 1: Conservative escape velocity analysis
        if a > self.thresholds['escape_au']:
            escape_factor = min((a - self.thresholds['escape_au']) / 2.0, 1.0)  # More conservative scaling
            escape_score = escape_factor * self.weights['heliocentric_escape']
            total_score += escape_score
            
            confidence_breakdown['heliocentric_escape'] = escape_score
            analysis['heliocentric_escape'] = {
                'semi_major_axis_au': a,
                'escape_factor': escape_factor,
                'score_contribution': escape_score,
                'reasoning': f"Object at {a:.3f} AU requires artificial propulsion (threshold: {self.thresholds['escape_au']} AU)"
            }
        
        # OPTIMIZED ANALYSIS 2: More selective launch azimuth analysis
        if i < self.thresholds['low_inclination']:
            launch_factor = (self.thresholds['low_inclination'] - i) / self.thresholds['low_inclination']
            # Apply exponential decay to reduce false positives from natural low-inclination objects
            launch_factor = launch_factor ** 1.5  # More selective
            launch_score = launch_factor * self.weights['launch_azimuth']
            total_score += launch_score
            
            confidence_breakdown['launch_azimuth'] = launch_score
            analysis['launch_azimuth'] = {
                'inclination_deg': i,
                'launch_factor': launch_factor,
                'score_contribution': launch_score,
                'reasoning': f"Low inclination {i:.1f}째 suggests launch optimization (threshold: <{self.thresholds['low_inclination']}째)"
            }
        
        # Enhanced polar orbit detection
        elif i > self.thresholds['high_inclination']:
            polar_factor = min((i - self.thresholds['high_inclination']) / 45, 1.0)
            polar_score = polar_factor * self.weights['polar_orbit']
            total_score += polar_score
            
            confidence_breakdown['polar_orbit'] = polar_score
            analysis['polar_orbit'] = {
                'inclination_deg': i,
                'polar_factor': polar_factor,
                'score_contribution': polar_score,
                'reasoning': f"High inclination {i:.1f}째 suggests artificial insertion (threshold: >{self.thresholds['high_inclination']}째)"
            }
        
        # OPTIMIZED ANALYSIS 3: More conservative energy analysis
        if e > self.thresholds['eccentricity']:
            # More conservative scoring with better natural object accommodation
            energy_factor = min((e - self.thresholds['eccentricity']) / 0.65, 1.0)
            # Apply square root to reduce impact of moderately eccentric natural objects
            energy_factor = math.sqrt(energy_factor)
            energy_score = energy_factor * self.weights['orbital_energy']
            total_score += energy_score
            
            confidence_breakdown['orbital_energy'] = energy_score
            analysis['orbital_energy'] = {
                'eccentricity': e,
                'energy_factor': energy_factor,
                'score_contribution': energy_score,
                'reasoning': f"High eccentricity {e:.3f} indicates artificial velocity changes (threshold: >{self.thresholds['eccentricity']})"
            }
        
        # OPTIMIZED ANALYSIS 4: Conservative brightness analysis
        if physical_data:
            diameter = physical_data.get('diameter', 0)
            magnitude = physical_data.get('absolute_magnitude', 0)
            
            if diameter > 0 and magnitude > 0:
                expected_mag = 5 * math.log10(diameter / 1000) + 15
                mag_difference = abs(magnitude - expected_mag)
                
                if mag_difference > self.thresholds['magnitude_deviation']:
                    brightness_factor = min((mag_difference - self.thresholds['magnitude_deviation']) / 7, 1.0)
                    brightness_score = brightness_factor * self.weights['brightness_anomaly']
                    total_score += brightness_score
                    
                    confidence_breakdown['brightness_anomaly'] = brightness_score
                    analysis['brightness_anomaly'] = {
                        'diameter_m': diameter,
                        'absolute_magnitude': magnitude,
                        'expected_magnitude': expected_mag,
                        'deviation': mag_difference,
                        'brightness_factor': brightness_factor,
                        'score_contribution': brightness_score,
                        'reasoning': f"Size-brightness deviation {mag_difference:.1f} mag inconsistent with natural objects"
                    }
        
        # OPTIMIZED ANALYSIS 5: More selective resonance detection
        if a > 0:
            period_years = a ** 1.5
            period_days = period_years * 365.25
            
            earth_period = 365.25
            resonance_ratios = [1.0, 2.0]  # Reduced to most significant resonances only
            
            for ratio in resonance_ratios:
                expected_period = earth_period * ratio
                period_difference = abs(period_days - expected_period) / expected_period
                
                if period_difference < self.thresholds['resonance_tolerance']:
                    resonance_factor = (self.thresholds['resonance_tolerance'] - period_difference) / self.thresholds['resonance_tolerance']
                    resonance_score = resonance_factor * self.weights['resonance']
                    total_score += resonance_score
                    
                    confidence_breakdown['resonance'] = resonance_score
                    analysis['resonance'] = {
                        'period_days': period_days,
                        'earth_ratio': ratio,
                        'resonance_factor': resonance_factor,
                        'score_contribution': resonance_score,
                        'reasoning': f"Orbital period near {ratio}:1 Earth resonance (within {period_difference:.1%})"
                    }
                    break
        
        # OPTIMIZED CONFIDENCE CALCULATION
        # Apply sigmoid function to provide better separation near threshold
        raw_confidence = min(total_score, 1.0)
        
        # Sigmoid transformation to enhance discrimination around threshold
        sigmoid_factor = 2.0 / (1.0 + math.exp(-8 * (raw_confidence - 0.5))) - 1.0
        confidence = max(0, min(1.0, sigmoid_factor))
        
        # Production threshold
        is_artificial = confidence > self.detection_threshold
        
        analysis['overall'] = {
            'raw_score': total_score,
            'sigmoid_confidence': confidence,
            'threshold': self.detection_threshold,
            'threshold_met': is_artificial,
            'analysis_components': len([k for k in analysis.keys() if k != 'overall']),
            'optimization_version': 'DELTA_OPTIMIZED_v1.0'
        }
        
        self.logger.info(f"Optimized analysis: {'ARTIFICIAL' if is_artificial else 'NATURAL'} "
                        f"(confidence: {confidence:.3f}, threshold: {self.detection_threshold}, "
                        f"components: {len(analysis)-1})")
        
        return OptimizedDetectionResult(
            is_artificial=is_artificial,
            confidence=confidence,
            analysis=analysis,
            confidence_breakdown=confidence_breakdown
        )
    
    def get_threshold_recommendations(self, validation_data: List[Dict]) -> Dict[str, float]:
        """Analyze validation data to recommend optimal thresholds."""
        
        thresholds = [0.5, 0.6, 0.7, 0.8]
        recommendations = {}
        
        for threshold in thresholds:
            # Simulate detection with this threshold
            artificial_detected = 0
            natural_detected = 0
            
            for obj_data in validation_data:
                result = self.analyze_neo(obj_data['orbital_elements'])
                
                if result.confidence > threshold:
                    if obj_data.get('type') == 'artificial_confirmed':
                        artificial_detected += 1
                    else:
                        natural_detected += 1
            
            artificial_objects = len([o for o in validation_data if o.get('type') == 'artificial_confirmed'])
            natural_objects = len(validation_data) - artificial_objects
            
            true_positive_rate = artificial_detected / artificial_objects if artificial_objects > 0 else 0
            false_positive_rate = natural_detected / natural_objects if natural_objects > 0 else 0
            
            recommendations[str(threshold)] = {
                'true_positive_rate': true_positive_rate,
                'false_positive_rate': false_positive_rate,
                'precision': artificial_detected / (artificial_detected + natural_detected) if (artificial_detected + natural_detected) > 0 else 0
            }
        
        return recommendations